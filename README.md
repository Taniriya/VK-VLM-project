# Проект по обучению русскоязычной визуально-языковой модели (VLM)

## Описание проекта

Данный проект направлен на разработку и оценку русскоязычной визуально-языковой модели на основе архитектуры LLaVA (Large Language and Vision Assistant). Проект использует открытые данные и модели, предоставленные командой VK, для создания решения, способного понимать и анализировать изображения в контексте русскоязычных текстовых запросов.

## Цель проекта

Основная цель проекта — достижение максимально высоких метрик производительности на русскоязычных бенчмарках **GQA-ru** и **MMBench-ru** путем дообучения предварительно обученной модели LLaVA на специализированных датасетах.

## Задачи проекта

1. **Загрузка и подготовка данных**: Использование открытых датасетов от VK для обучения и оценки модели
2. **Настройка модели**: Адаптация модели LLaVA-Saiga-8B с использованием эффективного метода дообучения LoRA (Low-Rank Adaptation)
3. **Обучение модели**: Дообучение модели на русскоязычных инструктивных данных для улучшения понимания контекста
4. **Оценка производительности**: Тестирование модели на стандартизированных бенчмарках для объективного измерения качества
5. **Анализ результатов**: Выявление слабых мест модели и определение направлений для дальнейшего улучшения

## Используемые данные от VK

Проект построен на основе следующих открытых датасетов и моделей от VK:

1. **LLaVA-Instruct-ru** - инструктивный датасет на русском языке, содержащий 109,905 примеров диалогов с изображениями, использован для обучения модели
2. **GQA-ru** - русскоязычная версия бенчмарка GQA, содержащая 12,216 вопросов по изображениям, использован для оценки точности модели
3. **MMBench-ru** - русскоязычная версия мультимодального бенчмарка, содержащая 3,910 разнообразных вопросов, использован для комплексной оценки
4. **LLaVA-Saiga-8B** - предварительно обученная визуально-языковая модель, адаптированная для русского языка

## Архитектура решения

### Техническая реализация

- **Базовая модель**: LLaVA-Saiga-8B с архитектурой, объединяющей визуальный энкодер и языковую модель
- **Метод дообучения**: LoRA (Low-Rank Adaptation) для эффективной адаптации больших моделей с минимальными вычислительными затратами
- **Точность вычислений**: Float16 для оптимального баланса между производительностью и точностью
- **Размер ранга LoRA**: 16 (параметр, определяющий сложность адаптации)

### Ключевые компоненты кода

1. **LLaVADataProcessor** - класс для обработки изображений и текстовых данных в формат, понятный модели
2. **Настройка модели с LoRA** - функция для инициализации модели с эффективной адаптацией
3. **Функция обучения** - полный пайплайн обучения модели на инструктивных данных
4. **Функция оценки** - комплексная оценка модели на бенчмарках GQA-ru и MMBench-ru
5. **BenchmarkOptimizer** - класс для анализа ошибок и генерации рекомендаций по улучшению

## Требования к вычислительным мощностям

Проект разработан с учетом ограничений по вычислительным ресурсам:

- **Минимальные требования**: GPU с 8 ГБ памяти (например, NVIDIA Tesla T4)
- **Рекомендуемые требования**: GPU с 16+ ГБ памяти для полноценного обучения
- **Оптимизации**: Использование 4-битной квантизации позволяет уменьшить потребление памяти в 4 раза
- **Альтернативы**: Код поддерживает работу как в режиме обучения, так и в режиме только оценки, что позволяет использовать его на различных конфигурациях оборудования

### Особенности реализации для разных сценариев:

1. **Оценка предобученной модели**: Может быть выполнена на GPU с 8 ГБ памяти
2. **Дообучение с LoRA**: Требует GPU с 12+ ГБ памяти для обработки обучающих данных
3. **Полное обучение**: Рекомендуется использовать GPU с 24+ ГБ памяти или распределенные вычисления

## Возможности кода

Предоставленный код позволяет:

1. **Загружать и предобрабатывать** специализированные датасеты для визуально-языкового моделирования
2. **Инициализировать и адаптировать** предварительно обученную модель LLaVA для русского языка
3. **Обучать модель** с использованием метода LoRA для эффективной адаптации
4. **Оценивать производительность** на стандартизированных русскоязычных бенчмарках
5. **Анализировать ошибки** и получать рекомендации по улучшению модели
6. **Генерировать подробные отчеты** о результатах эксперимента

## Результаты реализации

В ходе реализации проекта были получены следующие результаты:

### Текущие метрики производительности

- **GQA-ru**: 25.00% точности (5 правильных ответов из 20)
- **MMBench-ru**: 0.00% точности (0 правильных ответов из 20)

### Анализ ошибок

Анализ показал следующие основные проблемы:

1. **Несоответствие формата ответов**: Модель генерирует развернутые описания вместо кратких ответов, ожидаемых в бенчмарках
2. **Отсутствие изображений**: В предоставленных датасетах отсутствуют ссылки на реальные изображения, что ограничивает возможности мультимодального обучения
3. **Специфика бенчмарков**: MMBench-ru ожидает ответы в формате выбора вариантов (A, B, C, D), в то время как модель генерирует текстовые описания

### Материалы проекта

В результате выполнения проекта созданы следующие файлы:

1. **benchmark_results.json** - количественные результаты оценки модели
2. **error_analysis.json** - качественный анализ ошибок модели с примерами
3. **project_report.txt** - полный отчет о проекте с выводами и рекомендациями
4. **VK_VLM_project.ipynb** - основной исполняемый файл проекта

## Направления для улучшения

На основе анализа результатов определены следующие направления для улучшения модели:

1. **Дообучение на конкретных типах вопросов**: Специализированное обучение для вопросов, требующих подсчета, определения цвета или местоположения
2. **Настройка формата ответов**: Адаптация модели для генерации ответов в формате, соответствующем требованиям бенчмарков
3. **Увеличение размера обучающих данных**: Использование полной версии датасетов для более комплексного обучения
4. **Настройка гиперпараметров**: Оптимизация параметров обучения и генерации для улучшения точности
5. **Интеграция реальных изображений**: Внедрение механизмов работы с реальными изображениями для полноценного мультимодального обучения

## Заключение

Проект демонстрирует рабочий пайплайн для адаптации визуально-языковых моделей для русского языка. Несмотря на текущие ограничения в производительности, реализованное решение предоставляет основу для дальнейшего улучшения и может служить отправной точкой для разработки более совершенных русскоязычных VLM-моделей.

Код проекта является модульным и расширяемым, что позволяет легко адаптировать его для решения различных задач в области мультимодального искусственного интеллекта.
