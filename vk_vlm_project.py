# -*- coding: utf-8 -*-
"""VK-VLM-project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18bpIUfcSUgoDjQSeMTdV44ZHTz7Zo7C0
"""

#!pip install transformers accelerate datasets peft bitsandbytes pillow tqdm

import torch
import torch.nn as nn
from transformers import (
    AutoTokenizer,
    LlavaForConditionalGeneration,
    LlavaProcessor,
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset, DatasetDict
import pandas as pd
from PIL import Image
import requests
from io import BytesIO
import json
import os
from tqdm import tqdm
import numpy as np
from typing import Dict, List, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

# 1. ЗАГРУЗКА И ПОДГОТОВКА ДАННЫХ
print("="*60)
print("ИНИЦИАЛИЗАЦИЯ ПРОЕКТА VLM НА РУССКОМ ЯЗЫКЕ")
print("="*60)

def load_all_datasets():
    """Загрузка всех необходимых датасетов"""
    print("\n1. ЗАГРУЗКА ДАТАСЕТОВ...")

    datasets = {}

    try:
        # Инструктивный датасет для обучения
        print("Загрузка LLaVA-Instruct-ru...")
        instruct_dataset = load_dataset("deepvk/LLaVA-Instruct-ru", split="train")
        datasets["instruct"] = instruct_dataset
        print(f"  Загружено: {len(instruct_dataset)} примеров")

        # GQA-ru бенчмарк
        print("\nЗагрузка GQA-ru...")
        gqa_dataset = load_dataset("deepvk/GQA-ru", "testdev_balanced_instructions", split="testdev")
        datasets["gqa"] = gqa_dataset
        print(f"  Загружено: {len(gqa_dataset)} примеров")

        # MMBench-ru бенчмарк
        print("\nЗагрузка MMBench-ru...")
        mmbench_dataset = load_dataset("deepvk/MMBench-ru", split="dev")
        datasets["mmbench"] = mmbench_dataset
        print(f"  Загружено: {len(mmbench_dataset)} примеров")

        # Для демонстрации ограничим размеры датасетов
        if len(instruct_dataset) > 1000:
            datasets["instruct"] = instruct_dataset.select(range(1000))
            print(f"  Ограничено до: 1000 примеров для демо")

    except Exception as e:
        print(f"Ошибка при загрузке датасетов: {e}")
        print("Создаем минимальный демо-датасет...")

        # Создаем демо-датасет для тестирования
        demo_data = {
            "conversations": [[
                {"from": "human", "value": "Что изображено на картинке?"},
                {"from": "gpt", "value": "На картинке изображена кошка."}
            ] * 10],
            "image": ["http://example.com/cat.jpg"] * 10
        }
        datasets["instruct"] = DatasetDict({"train": demo_data})
        datasets["gqa"] = DatasetDict({"test": [{"question": "Какого цвета небо?", "answer": "синего"}] * 10})
        datasets["mmbench"] = DatasetDict({"test": [{"question": "Что больше: солнце или луна?", "answer": "солнце"}] * 10})

    return datasets

# 2. ПРЕПРОЦЕССИНГ ДЛЯ МОДЕЛИ LLaVA
class LLaVADataProcessor:
    """Процессор для подготовки данных для LLaVA модели"""

    def __init__(self, model_name: str = "deepvk/llava-saiga-8b"):
        print(f"\n2. ИНИЦИАЛИЗАЦИЯ ПРОЦЕССОРА ({model_name})...")

        try:
            self.processor = LlavaProcessor.from_pretrained(model_name)
            self.tokenizer = self.processor.tokenizer
            self.image_processor = self.processor.image_processor
            print("  Процессор успешно загружен")
        except:
            print("  Не удалось загрузить процессор LLaVA, используем базовый токенизатор")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.processor = None

    def prepare_instruct_example(self, example: Dict) -> Dict:
        """Подготовка примера для обучения"""
        try:
            conversations = example.get("conversations", [])

            # Форматируем диалог в текст
            texts = []
            for turn in conversations:
                role = "USER" if turn["from"] in ["human", "user"] else "ASSISTANT"
                texts.append(f"{role}: {turn['value']}")

            text = "\n".join(texts)

            # Если есть изображение
            image_url = example.get("image", "")
            image = None

            if image_url and isinstance(image_url, str) and image_url.startswith("http"):
                try:
                    response = requests.get(image_url, timeout=5)
                    image = Image.open(BytesIO(response.content)).convert("RGB")
                except:
                    image = None

            # Токенизация текста
            tokenized = self.tokenizer(
                text,
                truncation=True,
                padding="max_length",
                max_length=512,
                return_tensors="pt"
            )

            result = {
                "input_ids": tokenized["input_ids"][0],
                "attention_mask": tokenized["attention_mask"][0],
                "labels": tokenized["input_ids"][0].clone(),
            }

            if image and self.processor:
                # Обрабатываем изображение
                image_inputs = self.image_processor(image, return_tensors="pt")
                result["pixel_values"] = image_inputs["pixel_values"][0]

            return result

        except Exception as e:
            print(f"Ошибка при обработке примера: {e}")
            return None

    def prepare_benchmark_example(self, question: str, image_url: Optional[str] = None) -> Dict:
        """Подготовка примера для инференса"""
        prompt = f"USER: {question}\nASSISTANT:"

        inputs = {"text": prompt}

        if image_url and isinstance(image_url, str) and image_url.startswith("http"):
            try:
                response = requests.get(image_url, timeout=10)
                image = Image.open(BytesIO(response.content)).convert("RGB")
                inputs["images"] = [image]
            except Exception as e:
                print(f"Не удалось загрузить изображение: {e}")

        if self.processor and "images" in inputs:
            try:
                processed = self.processor(
                    inputs["text"],
                    inputs["images"],
                    return_tensors="pt",
                    padding=True
                )
                return processed
            except:
                pass

        # Fallback: только текст
        tokenized = self.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )

        return tokenized

# 3. НАСТРОЙКА МОДЕЛИ С LoRA
def setup_model_with_lora(model_name: str = "deepvk/llava-saiga-8b", use_lora: bool = True):
    """Инициализация модели с опциональной LoRA адаптацией"""
    print(f"\n3. ЗАГРУЗКА МОДЕЛИ {model_name}...")

    try:
        # Параметры для экономии памяти
        model_kwargs = {
            "torch_dtype": torch.float16,
            "device_map": "auto",
        }

        # Пробуем загрузить с 4-битной квантизацией
        try:
            model_kwargs["load_in_4bit"] = True
            model = LlavaForConditionalGeneration.from_pretrained(model_name, **model_kwargs)
            print("  Модель загружена в 4-битном режиме")
        except:
            model_kwargs.pop("load_in_4bit", None)
            model = LlavaForConditionalGeneration.from_pretrained(model_name, **model_kwargs)
            print("  Модель загружена в полной точности")

        if use_lora:
            print("  Настройка LoRA адаптации...")

            # Конфигурация LoRA
            lora_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                r=16,  # Размер ранга
                lora_alpha=32,
                lora_dropout=0.1,
                target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
                bias="none"
            )

            # Применяем LoRA
            model = get_peft_model(model, lora_config)
            model.print_trainable_parameters()

        print(f"  Модель успешно загружена на устройство: {next(model.parameters()).device}")
        return model

    except Exception as e:
        print(f"Ошибка при загрузке модели: {e}")
        raise

# 4. ФУНКЦИЯ ОБУЧЕНИЯ
def train_vlm_model(datasets: Dict, processor: LLaVADataProcessor, epochs: int = 1):
    """Обучение модели на инструктивных данных"""
    print(f"\n4. НАЧАЛО ОБУЧЕНИЯ ({epochs} эпох)...")

    # Подготовка данных
    print("  Подготовка тренировочных данных...")

    train_data = datasets.get("instruct", [])
    if hasattr(train_data, "select"):
        train_data = train_data.select(range(min(100, len(train_data))))

    processed_examples = []
    for i in tqdm(range(len(train_data)), desc="Обработка примеров"):
        example = train_data[i]
        processed = processor.prepare_instruct_example(example)
        if processed:
            processed_examples.append(processed)

    if not processed_examples:
        print("  Нет данных для обучения")
        return None, None

    # Создаем датасет
    from datasets import Dataset
    train_dataset = Dataset.from_list(processed_examples)

    # Разделяем на train/val
    split_dataset = train_dataset.train_test_split(test_size=0.1, seed=42)

    # Загружаем модель
    model = setup_model_with_lora(use_lora=True)

    # Параметры обучения
    training_args = TrainingArguments(
        output_dir="./llava-ru-finetuned",
        num_train_epochs=epochs,
        per_device_train_batch_size=2,
        per_device_eval_batch_size=2,
        warmup_steps=50,
        weight_decay=0.01,
        logging_dir="./logs",
        logging_steps=10,
        evaluation_strategy="steps",
        eval_steps=20,
        save_strategy="steps",
        save_steps=30,
        learning_rate=1e-4,
        fp16=True,
        gradient_accumulation_steps=4,
        report_to="none",
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="loss",
        greater_is_better=False
    )

    # Функция потерь
    class CustomTrainer(Trainer):
        def compute_loss(self, model, inputs, return_outputs=False):
            labels = inputs.pop("labels", None)
            outputs = model(**inputs)
            logits = outputs.logits

            if labels is not None:
                # Сдвигаем логиты и метки для вычисления потерь
                shift_logits = logits[..., :-1, :].contiguous()
                shift_labels = labels[..., 1:].contiguous()

                loss_fct = nn.CrossEntropyLoss()
                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),
                              shift_labels.view(-1))
            else:
                loss = outputs.loss

            return (loss, outputs) if return_outputs else loss

    # Создаем тренер
    trainer = CustomTrainer(
        model=model,
        args=training_args,
        train_dataset=split_dataset["train"],
        eval_dataset=split_dataset["test"],
        tokenizer=processor.tokenizer
    )

    # Обучаем
    print("  Запуск обучения...")
    train_result = trainer.train()

    # Сохраняем модель
    print("  Сохранение модели...")
    trainer.save_model("./llava-ru-finetuned-final")
    processor.tokenizer.save_pretrained("./llava-ru-finetuned-final")

    print("  Обучение завершено!")
    return model, processor

# 5. ФУНКЦИЯ ОЦЕНКИ НА БЕНЧМАРКАХ
def evaluate_on_benchmarks(model, processor: LLaVADataProcessor, datasets: Dict, max_samples: int = 20):
    """Оценка модели на русскоязычных бенчмарках"""
    print(f"\n5. ОЦЕНКА НА БЕНЧМАРКАХ ({max_samples} примеров каждый)...")

    model.eval()
    results = {}

    def generate_answer(question: str, image_url: Optional[str] = None) -> str:
        """Генерация ответа моделью"""
        try:
            # Подготавливаем входные данные
            inputs = processor.prepare_benchmark_example(question, image_url)

            # Переносим на устройство модели
            device = next(model.parameters()).device
            for key in inputs:
                if torch.is_tensor(inputs[key]):
                    inputs[key] = inputs[key].to(device)

            # Генерация
            with torch.no_grad():
                generate_ids = model.generate(
                    **inputs,
                    max_new_tokens=100,
                    temperature=0.1,
                    do_sample=False,
                    pad_token_id=processor.tokenizer.pad_token_id,
                    eos_token_id=processor.tokenizer.eos_token_id
                )

            # Декодируем ответ
            generated_text = processor.tokenizer.decode(
                generate_ids[0],
                skip_special_tokens=True
            )

            # Извлекаем ответ ассистента
            if "ASSISTANT:" in generated_text:
                answer = generated_text.split("ASSISTANT:")[-1].strip()
            elif "ASSISTANT" in generated_text:
                answer = generated_text.split("ASSISTANT")[-1].strip()
            else:
                answer = generated_text

            # Очищаем ответ
            answer = answer.split("USER:")[0].strip()
            answer = answer.split("\n")[0].strip()

            return answer

        except Exception as e:
            print(f"Ошибка генерации: {e}")
            return ""

    # Оценка на GQA-ru
    print("\n  ОЦЕНКА НА GQA-ru...")
    gqa_dataset = datasets.get("gqa", [])

    if hasattr(gqa_dataset, "select"):
        gqa_subset = gqa_dataset.select(range(min(max_samples, len(gqa_dataset))))
    else:
        gqa_subset = gqa_dataset[:max_samples] if len(gqa_dataset) > max_samples else gqa_dataset

    correct_gqa = 0
    total_gqa = len(gqa_subset)

    for i, example in enumerate(tqdm(gqa_subset, desc="GQA-ru")):
        question = example.get("question", "")
        true_answer = example.get("answer", "")

        if not question:
            continue

        # GQA-ru не содержит URL изображений, используем только текст
        predicted_answer = generate_answer(question)

        # Нормализация ответов для сравнения
        def normalize_answer(ans):
            if not isinstance(ans, str):
                return ""
            ans = ans.lower().strip()
            ans = ans.replace(".", "").replace(",", "")
            return ans

        norm_true = normalize_answer(true_answer)
        norm_pred = normalize_answer(predicted_answer)

        # Простое сравнение
        if norm_true and norm_pred:
            if (norm_true in norm_pred or
                norm_pred in norm_true or
                norm_true == norm_pred):
                correct_gqa += 1
            else:
                # Для отладки
                if i < 5:
                    print(f"    Пример {i+1}:")
                    print(f"      Вопрос: {question[:50]}...")
                    print(f"      Ожидалось: {true_answer}")
                    print(f"      Получено: {predicted_answer}")

    results["GQA-ru"] = {
        "accuracy": (correct_gqa / total_gqa * 100) if total_gqa > 0 else 0,
        "total_samples": total_gqa,
        "correct": correct_gqa
    }

    # Оценка на MMBench-ru
    print("\n  ОЦЕНКА НА MMBench-ru...")
    mmbench_dataset = datasets.get("mmbench", [])

    if hasattr(mmbench_dataset, "select"):
        mmbench_subset = mmbench_dataset.select(range(min(max_samples, len(mmbench_dataset))))
    else:
        mmbench_subset = mmbench_dataset[:max_samples] if len(mmbench_dataset) > max_samples else mmbench_dataset

    correct_mmbench = 0
    total_mmbench = len(mmbench_subset)

    for i, example in enumerate(tqdm(mmbench_subset, desc="MMBench-ru")):
        # MMBench может иметь другую структуру
        question = ""
        true_answer = ""

        if isinstance(example, dict):
            question = example.get("question", example.get("Question", ""))
            true_answer = example.get("answer", example.get("Answer", ""))

        if not question:
            continue

        predicted_answer = generate_answer(question)

        # Нормализация и сравнение
        norm_true = normalize_answer(true_answer)
        norm_pred = normalize_answer(predicted_answer)

        if norm_true and norm_pred:
            if (norm_true in norm_pred or
                norm_pred in norm_true or
                norm_true == norm_pred):
                correct_mmbench += 1
            else:
                # Для отладки
                if i < 5:
                    print(f"    Пример {i+1}:")
                    print(f"      Вопрос: {question[:50]}...")
                    print(f"      Ожидалось: {true_answer}")
                    print(f"      Получено: {predicted_answer}")

    results["MMBench-ru"] = {
        "accuracy": (correct_mmbench / total_mmbench * 100) if total_mmbench > 0 else 0,
        "total_samples": total_mmbench,
        "correct": correct_mmbench
    }

    # Сохраняем результаты
    with open("benchmark_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print("\n" + "="*50)
    print("РЕЗУЛЬТАТЫ ОЦЕНКИ:")
    for benchmark, metrics in results.items():
        print(f"  {benchmark}: {metrics['correct']}/{metrics['total_samples']} "
              f"({metrics['accuracy']:.2f}%)")
    print("="*50)

    return results

# 6. ОПТИМИЗАЦИЯ И АНАЛИЗ
class BenchmarkOptimizer:
    """Класс для оптимизации производительности на бенчмарках"""

    @staticmethod
    def analyze_errors(model, processor, datasets, num_examples: int = 10):
        """Анализ ошибок модели"""
        print("\n6. АНАЛИЗ ОШИБОК МОДЕЛИ...")

        error_examples = []

        # Проверяем несколько примеров из каждого бенчмарка
        for benchmark_name in ["gqa", "mmbench"]:
            dataset = datasets.get(benchmark_name, [])
            if not dataset:
                continue

            print(f"\n  Анализ {benchmark_name.upper()}...")

            for i in range(min(num_examples, len(dataset))):
                example = dataset[i]

                if benchmark_name == "gqa":
                    question = example.get("question", "")
                    true_answer = example.get("answer", "")
                else:
                    question = example.get("question", example.get("Question", ""))
                    true_answer = example.get("answer", example.get("Answer", ""))

                if not question:
                    continue

                # Генерируем ответ
                predicted_answer = ""
                try:
                    inputs = processor.prepare_benchmark_example(question)
                    device = next(model.parameters()).device
                    for key in inputs:
                        if torch.is_tensor(inputs[key]):
                            inputs[key] = inputs[key].to(device)

                    with torch.no_grad():
                        generate_ids = model.generate(
                            **inputs,
                            max_new_tokens=50,
                            temperature=0.1,
                            do_sample=False
                        )

                    generated_text = processor.tokenizer.decode(
                        generate_ids[0],
                        skip_special_tokens=True
                    )

                    if "ASSISTANT:" in generated_text:
                        predicted_answer = generated_text.split("ASSISTANT:")[-1].strip()

                except Exception as e:
                    predicted_answer = f"Ошибка: {e}"

                # Сравниваем
                true_norm = str(true_answer).lower().strip()
                pred_norm = str(predicted_answer).lower().strip()

                if (true_norm not in pred_norm and
                    pred_norm not in true_norm and
                    true_norm != pred_norm):

                    error_examples.append({
                        "benchmark": benchmark_name,
                        "question": question,
                        "true_answer": true_answer,
                        "predicted_answer": predicted_answer
                    })

                    print(f"    Ошибка #{len(error_examples)}:")
                    print(f"      Вопрос: {question[:60]}...")
                    print(f"      Ожидалось: {true_answer}")
                    print(f"      Получено: {predicted_answer[:60]}...")

        # Сохраняем ошибки для анализа
        if error_examples:
            with open("error_analysis.json", "w", encoding="utf-8") as f:
                json.dump(error_examples, f, indent=2, ensure_ascii=False)
            print(f"\n  Сохранено {len(error_examples)} примеров ошибок в error_analysis.json")

        return error_examples

    @staticmethod
    def get_improvement_suggestions(results: Dict, error_examples: List) -> str:
        """Генерация рекомендаций по улучшению"""
        suggestions = []

        gqa_acc = results.get("GQA-ru", {}).get("accuracy", 0)
        mmbench_acc = results.get("MMBench-ru", {}).get("accuracy", 0)

        if gqa_acc < 50:
            suggestions.append("1. Увеличить время обучения на LLaVA-Instruct-ru до 3-5 эпох")
            suggestions.append("2. Добавить data augmentation для текстовых вопросов")

        if mmbench_acc < 50:
            suggestions.append("3. Дообучить на разнообразных вопросах MMBench-ru")

        if error_examples:
            error_types = {}
            for error in error_examples:
                question = error["question"].lower()
                if "сколько" in question or "число" in question:
                    error_types["counting"] = error_types.get("counting", 0) + 1
                elif "какой цвет" in question or "цвет" in question:
                    error_types["color"] = error_types.get("color", 0) + 1
                elif "где" in question or "местоположение" in question:
                    error_types["location"] = error_types.get("location", 0) + 1

            for error_type, count in error_types.items():
                suggestions.append(f"4. Добавить специальные примеры для типа '{error_type}' ({count} ошибок)")

        suggestions.append("5. Использовать ансамблирование нескольких моделей")
        suggestions.append("6. Настроить температуру генерации (0.05 для точности, 0.7 для креативности)")
        suggestions.append("7. Увеличить размер ранга LoRA (r=32 или r=64)")

        return "\n".join(suggestions)

# 7. СОЗДАНИЕ ОТЧЕТА
def create_report(results: Dict, datasets: Dict, suggestions: str = ""):
    """Создание финального отчета проекта"""

    report = f"""
{'='*60}
ФИНАЛЬНЫЙ ОТЧЕТ ПРОЕКТА
{'='*60}
Дата: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
Устройство: {'GPU' if torch.cuda.is_available() else 'CPU'}

ЦЕЛЬ:
Получить максимальную метрику на русскоязычных бенчмарках
GQA-ru и MMBench-ru

{'='*60}
ИСПОЛЬЗОВАННЫЕ ДАТАСЕТЫ:
{'='*60}
1. LLaVA-Instruct-ru (обучение):
   • Примеров: {len(datasets.get('instruct', []))}
   • Тип: Инструктивные диалоги с изображениями

2. GQA-ru (тестирование):
   • Примеров: {len(datasets.get('gqa', []))}
   • Тип: Вопросы по изображениям

3. MMBench-ru (тестирование):
   • Примеров: {len(datasets.get('mmbench', []))}
   • Тип: Мультимодальные вопросы

{'='*60}
РЕЗУЛЬТАТЫ ОЦЕНКИ:
{'='*60}"""

    for benchmark, metrics in results.items():
        report += f"""
{benchmark}:
  • Точность: {metrics['accuracy']:.2f}%
  • Правильных ответов: {metrics['correct']}/{metrics['total_samples']}
  • Статус: {'✅ Хорошо' if metrics['accuracy'] > 60 else '⚠️ Нужно улучшение' if metrics['accuracy'] > 30 else '❌ Плохо'}"""

    report += f"""

{'='*60}
РЕКОМЕНДАЦИИ ПО УЛУЧШЕНИЮ:
{'='*60}
{suggestions}

{'='*60}
СЛЕДУЮЩИЕ ШАГИ:
{'='*60}
1. Увеличить размер тренировочных данных
2. Использовать полную версию датасетов
3. Применить продвинутые техники аугментации
4. Настроить гиперпараметры обучения
5. Протестировать на реальных изображениях

{'='*60}
АРХИТЕКТУРА РЕШЕНИЯ:
{'='*60}
• Базовая модель: LLaVA-Saiga-8B
• Метод адаптации: LoRA (Low-Rank Adaptation)
• Точность: float16
• Размер ранга LoRA: 16
• Обучение: Инструктивное fine-tuning
"""

    # Сохраняем отчет
    with open("project_report.txt", "w", encoding="utf-8") as f:
        f.write(report)

    print(report)
    return report

# 8. ОСНОВНОЙ СЦЕНАРИЙ ВЫПОЛНЕНИЯ
def main(training_mode: bool = False, max_samples: int = 20):
    """Полный пайплайн проекта"""

    print("="*60)
    print("ЗАПУСК ПРОЕКТА: Обучение русскоязычной VLM модели")
    print("="*60)

    # Проверяем доступность GPU
    print(f"\nПРОВЕРКА ОБОРУДОВАНИЯ:")
    print(f"  GPU доступен: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  GPU: {torch.cuda.get_device_name(0)}")
        print(f"  Память GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    else:
        print("  ⚠️  Внимание: GPU не найден, обучение может быть медленным")

    # Шаг 1: Загрузка данных
    datasets = load_all_datasets()

    # Шаг 2: Инициализация процессора
    processor = LLaVADataProcessor()

    # Шаг 3: Обучение или загрузка модели
    if training_mode:
        print("\n" + "="*60)
        print("РЕЖИМ: ОБУЧЕНИЕ НОВОЙ МОДЕЛИ")
        print("="*60)
        model, processor = train_vlm_model(datasets, processor, epochs=1)
        if model is None:
            print("Не удалось обучить модель, используем предобученную")
            model = setup_model_with_lora(use_lora=False)
    else:
        print("\n" + "="*60)
        print("РЕЖИМ: ТЕСТИРОВАНИЕ ПРЕДОБУЧЕННОЙ МОДЕЛИ")
        print("="*60)
        model = setup_model_with_lora(use_lora=False)

    # Шаг 4: Оценка на бенчмарках
    results = evaluate_on_benchmarks(model, processor, datasets, max_samples)

    # Шаг 5: Анализ и оптимизация
    optimizer = BenchmarkOptimizer()
    error_examples = optimizer.analyze_errors(model, processor, datasets, num_examples=5)
    suggestions = optimizer.get_improvement_suggestions(results, error_examples)

    # Шаг 6: Создание отчета
    report = create_report(results, datasets, suggestions)

    print("\n" + "="*60)
    print("ПРОЕКТ УСПЕШНО ЗАВЕРШЕН!")
    print("="*60)
    print("Созданные файлы:")
    print("  1. benchmark_results.json - результаты оценки")
    print("  2. error_analysis.json - анализ ошибок")
    print("  3. project_report.txt - полный отчет")
    if training_mode:
        print("  4. llava-ru-finetuned-final/ - дообученная модель")
    print("="*60)

    return model, processor, results

# 9. ЗАПУСК ПРОЕКТА
if __name__ == "__main__":
    # Параметры запуска
    TRAINING_MODE = False  # Поставьте True для обучения модели
    MAX_SAMPLES = 20      # Количество примеров для оценки

    try:
        model, processor, results = main(
            training_mode=TRAINING_MODE,
            max_samples=MAX_SAMPLES
        )

        # Дополнительная информация
        print("\nДОПОЛНИТЕЛЬНАЯ ИНФОРМАЦИЯ:")
        print(f"  Размер модели: {sum(p.numel() for p in model.parameters()):,} параметров")
        print(f"  Точность лучшего бенчмарка: {max(r['accuracy'] for r in results.values()):.2f}%")

    except Exception as e:
        print(f"\n❌ ОШИБКА ВЫПОЛНЕНИЯ: {e}")
        print("\nСОВЕТЫ ПО УСТРАНЕНИЮ:")
        print("1. Проверьте подключение к интернету")
        print("2. Убедитесь, что есть достаточно памяти GPU (минимум 8GB)")
        print("3. Уменьшите MAX_SAMPLES если памяти мало")
        print("4. Используйте Google Colab с GPU T4 или лучше")

        # Минимальный отчет при ошибке
        try:
            with open("error_log.txt", "w") as f:
                import traceback
                f.write(traceback.format_exc())
            print("Лог ошибки сохранен в error_log.txt")
        except:
            pass