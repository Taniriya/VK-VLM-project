{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh0it6TRvzoi",
        "outputId": "75d4f052-c3a8-4e31-936c-7bab666451be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.49.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers accelerate datasets peft bitsandbytes pillow tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    LlavaForConditionalGeneration,\n",
        "    LlavaProcessor,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 1. ЗАГРУЗКА И ПОДГОТОВКА ДАННЫХ\n",
        "print(\"=\"*60)\n",
        "print(\"ИНИЦИАЛИЗАЦИЯ ПРОЕКТА VLM НА РУССКОМ ЯЗЫКЕ\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def load_all_datasets():\n",
        "    \"\"\"Загрузка всех необходимых датасетов\"\"\"\n",
        "    print(\"\\n1. ЗАГРУЗКА ДАТАСЕТОВ...\")\n",
        "\n",
        "    datasets = {}\n",
        "\n",
        "    try:\n",
        "        # Инструктивный датасет для обучения\n",
        "        print(\"Загрузка LLaVA-Instruct-ru...\")\n",
        "        instruct_dataset = load_dataset(\"deepvk/LLaVA-Instruct-ru\", split=\"train\")\n",
        "        datasets[\"instruct\"] = instruct_dataset\n",
        "        print(f\"  Загружено: {len(instruct_dataset)} примеров\")\n",
        "\n",
        "        # GQA-ru бенчмарк\n",
        "        print(\"\\nЗагрузка GQA-ru...\")\n",
        "        gqa_dataset = load_dataset(\"deepvk/GQA-ru\", \"testdev_balanced_instructions\", split=\"testdev\")\n",
        "        datasets[\"gqa\"] = gqa_dataset\n",
        "        print(f\"  Загружено: {len(gqa_dataset)} примеров\")\n",
        "\n",
        "        # MMBench-ru бенчмарк\n",
        "        print(\"\\nЗагрузка MMBench-ru...\")\n",
        "        mmbench_dataset = load_dataset(\"deepvk/MMBench-ru\", split=\"dev\")\n",
        "        datasets[\"mmbench\"] = mmbench_dataset\n",
        "        print(f\"  Загружено: {len(mmbench_dataset)} примеров\")\n",
        "\n",
        "        # Для демонстрации ограничим размеры датасетов\n",
        "        if len(instruct_dataset) > 1000:\n",
        "            datasets[\"instruct\"] = instruct_dataset.select(range(1000))\n",
        "            print(f\"  Ограничено до: 1000 примеров для демо\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при загрузке датасетов: {e}\")\n",
        "        print(\"Создаем минимальный демо-датасет...\")\n",
        "\n",
        "        # Создаем демо-датасет для тестирования\n",
        "        demo_data = {\n",
        "            \"conversations\": [[\n",
        "                {\"from\": \"human\", \"value\": \"Что изображено на картинке?\"},\n",
        "                {\"from\": \"gpt\", \"value\": \"На картинке изображена кошка.\"}\n",
        "            ] * 10],\n",
        "            \"image\": [\"http://example.com/cat.jpg\"] * 10\n",
        "        }\n",
        "        datasets[\"instruct\"] = DatasetDict({\"train\": demo_data})\n",
        "        datasets[\"gqa\"] = DatasetDict({\"test\": [{\"question\": \"Какого цвета небо?\", \"answer\": \"синего\"}] * 10})\n",
        "        datasets[\"mmbench\"] = DatasetDict({\"test\": [{\"question\": \"Что больше: солнце или луна?\", \"answer\": \"солнце\"}] * 10})\n",
        "\n",
        "    return datasets\n",
        "\n",
        "# 2. ПРЕПРОЦЕССИНГ ДЛЯ МОДЕЛИ LLaVA\n",
        "class LLaVADataProcessor:\n",
        "    \"\"\"Процессор для подготовки данных для LLaVA модели\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"deepvk/llava-saiga-8b\"):\n",
        "        print(f\"\\n2. ИНИЦИАЛИЗАЦИЯ ПРОЦЕССОРА ({model_name})...\")\n",
        "\n",
        "        try:\n",
        "            self.processor = LlavaProcessor.from_pretrained(model_name)\n",
        "            self.tokenizer = self.processor.tokenizer\n",
        "            self.image_processor = self.processor.image_processor\n",
        "            print(\"  Процессор успешно загружен\")\n",
        "        except:\n",
        "            print(\"  Не удалось загрузить процессор LLaVA, используем базовый токенизатор\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            self.processor = None\n",
        "\n",
        "    def prepare_instruct_example(self, example: Dict) -> Dict:\n",
        "        \"\"\"Подготовка примера для обучения\"\"\"\n",
        "        try:\n",
        "            conversations = example.get(\"conversations\", [])\n",
        "\n",
        "            # Форматируем диалог в текст\n",
        "            texts = []\n",
        "            for turn in conversations:\n",
        "                role = \"USER\" if turn[\"from\"] in [\"human\", \"user\"] else \"ASSISTANT\"\n",
        "                texts.append(f\"{role}: {turn['value']}\")\n",
        "\n",
        "            text = \"\\n\".join(texts)\n",
        "\n",
        "            # Если есть изображение\n",
        "            image_url = example.get(\"image\", \"\")\n",
        "            image = None\n",
        "\n",
        "            if image_url and isinstance(image_url, str) and image_url.startswith(\"http\"):\n",
        "                try:\n",
        "                    response = requests.get(image_url, timeout=5)\n",
        "                    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "                except:\n",
        "                    image = None\n",
        "\n",
        "            # Токенизация текста\n",
        "            tokenized = self.tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=512,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            result = {\n",
        "                \"input_ids\": tokenized[\"input_ids\"][0],\n",
        "                \"attention_mask\": tokenized[\"attention_mask\"][0],\n",
        "                \"labels\": tokenized[\"input_ids\"][0].clone(),\n",
        "            }\n",
        "\n",
        "            if image and self.processor:\n",
        "                # Обрабатываем изображение\n",
        "                image_inputs = self.image_processor(image, return_tensors=\"pt\")\n",
        "                result[\"pixel_values\"] = image_inputs[\"pixel_values\"][0]\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при обработке примера: {e}\")\n",
        "            return None\n",
        "\n",
        "    def prepare_benchmark_example(self, question: str, image_url: Optional[str] = None) -> Dict:\n",
        "        \"\"\"Подготовка примера для инференса\"\"\"\n",
        "        prompt = f\"USER: {question}\\nASSISTANT:\"\n",
        "\n",
        "        inputs = {\"text\": prompt}\n",
        "\n",
        "        if image_url and isinstance(image_url, str) and image_url.startswith(\"http\"):\n",
        "            try:\n",
        "                response = requests.get(image_url, timeout=10)\n",
        "                image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "                inputs[\"images\"] = [image]\n",
        "            except Exception as e:\n",
        "                print(f\"Не удалось загрузить изображение: {e}\")\n",
        "\n",
        "        if self.processor and \"images\" in inputs:\n",
        "            try:\n",
        "                processed = self.processor(\n",
        "                    inputs[\"text\"],\n",
        "                    inputs[\"images\"],\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True\n",
        "                )\n",
        "                return processed\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Fallback: только текст\n",
        "        tokenized = self.tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "        return tokenized\n",
        "\n",
        "# 3. НАСТРОЙКА МОДЕЛИ С LoRA\n",
        "def setup_model_with_lora(model_name: str = \"deepvk/llava-saiga-8b\", use_lora: bool = True):\n",
        "    \"\"\"Инициализация модели с опциональной LoRA адаптацией\"\"\"\n",
        "    print(f\"\\n3. ЗАГРУЗКА МОДЕЛИ {model_name}...\")\n",
        "\n",
        "    try:\n",
        "        # Параметры для экономии памяти\n",
        "        model_kwargs = {\n",
        "            \"torch_dtype\": torch.float16,\n",
        "            \"device_map\": \"auto\",\n",
        "        }\n",
        "\n",
        "        # Пробуем загрузить с 4-битной квантизацией\n",
        "        try:\n",
        "            model_kwargs[\"load_in_4bit\"] = True\n",
        "            model = LlavaForConditionalGeneration.from_pretrained(model_name, **model_kwargs)\n",
        "            print(\"  Модель загружена в 4-битном режиме\")\n",
        "        except:\n",
        "            model_kwargs.pop(\"load_in_4bit\", None)\n",
        "            model = LlavaForConditionalGeneration.from_pretrained(model_name, **model_kwargs)\n",
        "            print(\"  Модель загружена в полной точности\")\n",
        "\n",
        "        if use_lora:\n",
        "            print(\"  Настройка LoRA адаптации...\")\n",
        "\n",
        "            # Конфигурация LoRA\n",
        "            lora_config = LoraConfig(\n",
        "                task_type=TaskType.CAUSAL_LM,\n",
        "                r=16,  # Размер ранга\n",
        "                lora_alpha=32,\n",
        "                lora_dropout=0.1,\n",
        "                target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "                bias=\"none\"\n",
        "            )\n",
        "\n",
        "            # Применяем LoRA\n",
        "            model = get_peft_model(model, lora_config)\n",
        "            model.print_trainable_parameters()\n",
        "\n",
        "        print(f\"  Модель успешно загружена на устройство: {next(model.parameters()).device}\")\n",
        "        return model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при загрузке модели: {e}\")\n",
        "        raise\n",
        "\n",
        "# 4. ФУНКЦИЯ ОБУЧЕНИЯ\n",
        "def train_vlm_model(datasets: Dict, processor: LLaVADataProcessor, epochs: int = 1):\n",
        "    \"\"\"Обучение модели на инструктивных данных\"\"\"\n",
        "    print(f\"\\n4. НАЧАЛО ОБУЧЕНИЯ ({epochs} эпох)...\")\n",
        "\n",
        "    # Подготовка данных\n",
        "    print(\"  Подготовка тренировочных данных...\")\n",
        "\n",
        "    train_data = datasets.get(\"instruct\", [])\n",
        "    if hasattr(train_data, \"select\"):\n",
        "        train_data = train_data.select(range(min(100, len(train_data))))\n",
        "\n",
        "    processed_examples = []\n",
        "    for i in tqdm(range(len(train_data)), desc=\"Обработка примеров\"):\n",
        "        example = train_data[i]\n",
        "        processed = processor.prepare_instruct_example(example)\n",
        "        if processed:\n",
        "            processed_examples.append(processed)\n",
        "\n",
        "    if not processed_examples:\n",
        "        print(\"  Нет данных для обучения\")\n",
        "        return None, None\n",
        "\n",
        "    # Создаем датасет\n",
        "    from datasets import Dataset\n",
        "    train_dataset = Dataset.from_list(processed_examples)\n",
        "\n",
        "    # Разделяем на train/val\n",
        "    split_dataset = train_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "    # Загружаем модель\n",
        "    model = setup_model_with_lora(use_lora=True)\n",
        "\n",
        "    # Параметры обучения\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./llava-ru-finetuned\",\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        warmup_steps=50,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=20,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=30,\n",
        "        learning_rate=1e-4,\n",
        "        fp16=True,\n",
        "        gradient_accumulation_steps=4,\n",
        "        report_to=\"none\",\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"loss\",\n",
        "        greater_is_better=False\n",
        "    )\n",
        "\n",
        "    # Функция потерь\n",
        "    class CustomTrainer(Trainer):\n",
        "        def compute_loss(self, model, inputs, return_outputs=False):\n",
        "            labels = inputs.pop(\"labels\", None)\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            if labels is not None:\n",
        "                # Сдвигаем логиты и метки для вычисления потерь\n",
        "                shift_logits = logits[..., :-1, :].contiguous()\n",
        "                shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "                loss_fct = nn.CrossEntropyLoss()\n",
        "                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n",
        "                              shift_labels.view(-1))\n",
        "            else:\n",
        "                loss = outputs.loss\n",
        "\n",
        "            return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    # Создаем тренер\n",
        "    trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=split_dataset[\"train\"],\n",
        "        eval_dataset=split_dataset[\"test\"],\n",
        "        tokenizer=processor.tokenizer\n",
        "    )\n",
        "\n",
        "    # Обучаем\n",
        "    print(\"  Запуск обучения...\")\n",
        "    train_result = trainer.train()\n",
        "\n",
        "    # Сохраняем модель\n",
        "    print(\"  Сохранение модели...\")\n",
        "    trainer.save_model(\"./llava-ru-finetuned-final\")\n",
        "    processor.tokenizer.save_pretrained(\"./llava-ru-finetuned-final\")\n",
        "\n",
        "    print(\"  Обучение завершено!\")\n",
        "    return model, processor\n",
        "\n",
        "# 5. ФУНКЦИЯ ОЦЕНКИ НА БЕНЧМАРКАХ\n",
        "def evaluate_on_benchmarks(model, processor: LLaVADataProcessor, datasets: Dict, max_samples: int = 20):\n",
        "    \"\"\"Оценка модели на русскоязычных бенчмарках\"\"\"\n",
        "    print(f\"\\n5. ОЦЕНКА НА БЕНЧМАРКАХ ({max_samples} примеров каждый)...\")\n",
        "\n",
        "    model.eval()\n",
        "    results = {}\n",
        "\n",
        "    def generate_answer(question: str, image_url: Optional[str] = None) -> str:\n",
        "        \"\"\"Генерация ответа моделью\"\"\"\n",
        "        try:\n",
        "            # Подготавливаем входные данные\n",
        "            inputs = processor.prepare_benchmark_example(question, image_url)\n",
        "\n",
        "            # Переносим на устройство модели\n",
        "            device = next(model.parameters()).device\n",
        "            for key in inputs:\n",
        "                if torch.is_tensor(inputs[key]):\n",
        "                    inputs[key] = inputs[key].to(device)\n",
        "\n",
        "            # Генерация\n",
        "            with torch.no_grad():\n",
        "                generate_ids = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=100,\n",
        "                    temperature=0.1,\n",
        "                    do_sample=False,\n",
        "                    pad_token_id=processor.tokenizer.pad_token_id,\n",
        "                    eos_token_id=processor.tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "            # Декодируем ответ\n",
        "            generated_text = processor.tokenizer.decode(\n",
        "                generate_ids[0],\n",
        "                skip_special_tokens=True\n",
        "            )\n",
        "\n",
        "            # Извлекаем ответ ассистента\n",
        "            if \"ASSISTANT:\" in generated_text:\n",
        "                answer = generated_text.split(\"ASSISTANT:\")[-1].strip()\n",
        "            elif \"ASSISTANT\" in generated_text:\n",
        "                answer = generated_text.split(\"ASSISTANT\")[-1].strip()\n",
        "            else:\n",
        "                answer = generated_text\n",
        "\n",
        "            # Очищаем ответ\n",
        "            answer = answer.split(\"USER:\")[0].strip()\n",
        "            answer = answer.split(\"\\n\")[0].strip()\n",
        "\n",
        "            return answer\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка генерации: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    # Оценка на GQA-ru\n",
        "    print(\"\\n  ОЦЕНКА НА GQA-ru...\")\n",
        "    gqa_dataset = datasets.get(\"gqa\", [])\n",
        "\n",
        "    if hasattr(gqa_dataset, \"select\"):\n",
        "        gqa_subset = gqa_dataset.select(range(min(max_samples, len(gqa_dataset))))\n",
        "    else:\n",
        "        gqa_subset = gqa_dataset[:max_samples] if len(gqa_dataset) > max_samples else gqa_dataset\n",
        "\n",
        "    correct_gqa = 0\n",
        "    total_gqa = len(gqa_subset)\n",
        "\n",
        "    for i, example in enumerate(tqdm(gqa_subset, desc=\"GQA-ru\")):\n",
        "        question = example.get(\"question\", \"\")\n",
        "        true_answer = example.get(\"answer\", \"\")\n",
        "\n",
        "        if not question:\n",
        "            continue\n",
        "\n",
        "        # GQA-ru не содержит URL изображений, используем только текст\n",
        "        predicted_answer = generate_answer(question)\n",
        "\n",
        "        # Нормализация ответов для сравнения\n",
        "        def normalize_answer(ans):\n",
        "            if not isinstance(ans, str):\n",
        "                return \"\"\n",
        "            ans = ans.lower().strip()\n",
        "            ans = ans.replace(\".\", \"\").replace(\",\", \"\")\n",
        "            return ans\n",
        "\n",
        "        norm_true = normalize_answer(true_answer)\n",
        "        norm_pred = normalize_answer(predicted_answer)\n",
        "\n",
        "        # Простое сравнение\n",
        "        if norm_true and norm_pred:\n",
        "            if (norm_true in norm_pred or\n",
        "                norm_pred in norm_true or\n",
        "                norm_true == norm_pred):\n",
        "                correct_gqa += 1\n",
        "            else:\n",
        "                # Для отладки\n",
        "                if i < 5:\n",
        "                    print(f\"    Пример {i+1}:\")\n",
        "                    print(f\"      Вопрос: {question[:50]}...\")\n",
        "                    print(f\"      Ожидалось: {true_answer}\")\n",
        "                    print(f\"      Получено: {predicted_answer}\")\n",
        "\n",
        "    results[\"GQA-ru\"] = {\n",
        "        \"accuracy\": (correct_gqa / total_gqa * 100) if total_gqa > 0 else 0,\n",
        "        \"total_samples\": total_gqa,\n",
        "        \"correct\": correct_gqa\n",
        "    }\n",
        "\n",
        "    # Оценка на MMBench-ru\n",
        "    print(\"\\n  ОЦЕНКА НА MMBench-ru...\")\n",
        "    mmbench_dataset = datasets.get(\"mmbench\", [])\n",
        "\n",
        "    if hasattr(mmbench_dataset, \"select\"):\n",
        "        mmbench_subset = mmbench_dataset.select(range(min(max_samples, len(mmbench_dataset))))\n",
        "    else:\n",
        "        mmbench_subset = mmbench_dataset[:max_samples] if len(mmbench_dataset) > max_samples else mmbench_dataset\n",
        "\n",
        "    correct_mmbench = 0\n",
        "    total_mmbench = len(mmbench_subset)\n",
        "\n",
        "    for i, example in enumerate(tqdm(mmbench_subset, desc=\"MMBench-ru\")):\n",
        "        # MMBench может иметь другую структуру\n",
        "        question = \"\"\n",
        "        true_answer = \"\"\n",
        "\n",
        "        if isinstance(example, dict):\n",
        "            question = example.get(\"question\", example.get(\"Question\", \"\"))\n",
        "            true_answer = example.get(\"answer\", example.get(\"Answer\", \"\"))\n",
        "\n",
        "        if not question:\n",
        "            continue\n",
        "\n",
        "        predicted_answer = generate_answer(question)\n",
        "\n",
        "        # Нормализация и сравнение\n",
        "        norm_true = normalize_answer(true_answer)\n",
        "        norm_pred = normalize_answer(predicted_answer)\n",
        "\n",
        "        if norm_true and norm_pred:\n",
        "            if (norm_true in norm_pred or\n",
        "                norm_pred in norm_true or\n",
        "                norm_true == norm_pred):\n",
        "                correct_mmbench += 1\n",
        "            else:\n",
        "                # Для отладки\n",
        "                if i < 5:\n",
        "                    print(f\"    Пример {i+1}:\")\n",
        "                    print(f\"      Вопрос: {question[:50]}...\")\n",
        "                    print(f\"      Ожидалось: {true_answer}\")\n",
        "                    print(f\"      Получено: {predicted_answer}\")\n",
        "\n",
        "    results[\"MMBench-ru\"] = {\n",
        "        \"accuracy\": (correct_mmbench / total_mmbench * 100) if total_mmbench > 0 else 0,\n",
        "        \"total_samples\": total_mmbench,\n",
        "        \"correct\": correct_mmbench\n",
        "    }\n",
        "\n",
        "    # Сохраняем результаты\n",
        "    with open(\"benchmark_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"РЕЗУЛЬТАТЫ ОЦЕНКИ:\")\n",
        "    for benchmark, metrics in results.items():\n",
        "        print(f\"  {benchmark}: {metrics['correct']}/{metrics['total_samples']} \"\n",
        "              f\"({metrics['accuracy']:.2f}%)\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    return results\n",
        "\n",
        "# 6. ОПТИМИЗАЦИЯ И АНАЛИЗ\n",
        "class BenchmarkOptimizer:\n",
        "    \"\"\"Класс для оптимизации производительности на бенчмарках\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def analyze_errors(model, processor, datasets, num_examples: int = 10):\n",
        "        \"\"\"Анализ ошибок модели\"\"\"\n",
        "        print(\"\\n6. АНАЛИЗ ОШИБОК МОДЕЛИ...\")\n",
        "\n",
        "        error_examples = []\n",
        "\n",
        "        # Проверяем несколько примеров из каждого бенчмарка\n",
        "        for benchmark_name in [\"gqa\", \"mmbench\"]:\n",
        "            dataset = datasets.get(benchmark_name, [])\n",
        "            if not dataset:\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n  Анализ {benchmark_name.upper()}...\")\n",
        "\n",
        "            for i in range(min(num_examples, len(dataset))):\n",
        "                example = dataset[i]\n",
        "\n",
        "                if benchmark_name == \"gqa\":\n",
        "                    question = example.get(\"question\", \"\")\n",
        "                    true_answer = example.get(\"answer\", \"\")\n",
        "                else:\n",
        "                    question = example.get(\"question\", example.get(\"Question\", \"\"))\n",
        "                    true_answer = example.get(\"answer\", example.get(\"Answer\", \"\"))\n",
        "\n",
        "                if not question:\n",
        "                    continue\n",
        "\n",
        "                # Генерируем ответ\n",
        "                predicted_answer = \"\"\n",
        "                try:\n",
        "                    inputs = processor.prepare_benchmark_example(question)\n",
        "                    device = next(model.parameters()).device\n",
        "                    for key in inputs:\n",
        "                        if torch.is_tensor(inputs[key]):\n",
        "                            inputs[key] = inputs[key].to(device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        generate_ids = model.generate(\n",
        "                            **inputs,\n",
        "                            max_new_tokens=50,\n",
        "                            temperature=0.1,\n",
        "                            do_sample=False\n",
        "                        )\n",
        "\n",
        "                    generated_text = processor.tokenizer.decode(\n",
        "                        generate_ids[0],\n",
        "                        skip_special_tokens=True\n",
        "                    )\n",
        "\n",
        "                    if \"ASSISTANT:\" in generated_text:\n",
        "                        predicted_answer = generated_text.split(\"ASSISTANT:\")[-1].strip()\n",
        "\n",
        "                except Exception as e:\n",
        "                    predicted_answer = f\"Ошибка: {e}\"\n",
        "\n",
        "                # Сравниваем\n",
        "                true_norm = str(true_answer).lower().strip()\n",
        "                pred_norm = str(predicted_answer).lower().strip()\n",
        "\n",
        "                if (true_norm not in pred_norm and\n",
        "                    pred_norm not in true_norm and\n",
        "                    true_norm != pred_norm):\n",
        "\n",
        "                    error_examples.append({\n",
        "                        \"benchmark\": benchmark_name,\n",
        "                        \"question\": question,\n",
        "                        \"true_answer\": true_answer,\n",
        "                        \"predicted_answer\": predicted_answer\n",
        "                    })\n",
        "\n",
        "                    print(f\"    Ошибка #{len(error_examples)}:\")\n",
        "                    print(f\"      Вопрос: {question[:60]}...\")\n",
        "                    print(f\"      Ожидалось: {true_answer}\")\n",
        "                    print(f\"      Получено: {predicted_answer[:60]}...\")\n",
        "\n",
        "        # Сохраняем ошибки для анализа\n",
        "        if error_examples:\n",
        "            with open(\"error_analysis.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(error_examples, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"\\n  Сохранено {len(error_examples)} примеров ошибок в error_analysis.json\")\n",
        "\n",
        "        return error_examples\n",
        "\n",
        "    @staticmethod\n",
        "    def get_improvement_suggestions(results: Dict, error_examples: List) -> str:\n",
        "        \"\"\"Генерация рекомендаций по улучшению\"\"\"\n",
        "        suggestions = []\n",
        "\n",
        "        gqa_acc = results.get(\"GQA-ru\", {}).get(\"accuracy\", 0)\n",
        "        mmbench_acc = results.get(\"MMBench-ru\", {}).get(\"accuracy\", 0)\n",
        "\n",
        "        if gqa_acc < 50:\n",
        "            suggestions.append(\"1. Увеличить время обучения на LLaVA-Instruct-ru до 3-5 эпох\")\n",
        "            suggestions.append(\"2. Добавить data augmentation для текстовых вопросов\")\n",
        "\n",
        "        if mmbench_acc < 50:\n",
        "            suggestions.append(\"3. Дообучить на разнообразных вопросах MMBench-ru\")\n",
        "\n",
        "        if error_examples:\n",
        "            error_types = {}\n",
        "            for error in error_examples:\n",
        "                question = error[\"question\"].lower()\n",
        "                if \"сколько\" in question or \"число\" in question:\n",
        "                    error_types[\"counting\"] = error_types.get(\"counting\", 0) + 1\n",
        "                elif \"какой цвет\" in question or \"цвет\" in question:\n",
        "                    error_types[\"color\"] = error_types.get(\"color\", 0) + 1\n",
        "                elif \"где\" in question or \"местоположение\" in question:\n",
        "                    error_types[\"location\"] = error_types.get(\"location\", 0) + 1\n",
        "\n",
        "            for error_type, count in error_types.items():\n",
        "                suggestions.append(f\"4. Добавить специальные примеры для типа '{error_type}' ({count} ошибок)\")\n",
        "\n",
        "        suggestions.append(\"5. Использовать ансамблирование нескольких моделей\")\n",
        "        suggestions.append(\"6. Настроить температуру генерации (0.05 для точности, 0.7 для креативности)\")\n",
        "        suggestions.append(\"7. Увеличить размер ранга LoRA (r=32 или r=64)\")\n",
        "\n",
        "        return \"\\n\".join(suggestions)\n",
        "\n",
        "# 7. СОЗДАНИЕ ОТЧЕТА\n",
        "def create_report(results: Dict, datasets: Dict, suggestions: str = \"\"):\n",
        "    \"\"\"Создание финального отчета проекта\"\"\"\n",
        "\n",
        "    report = f\"\"\"\n",
        "{'='*60}\n",
        "ФИНАЛЬНЫЙ ОТЧЕТ ПРОЕКТА\n",
        "{'='*60}\n",
        "Дата: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "Устройство: {'GPU' if torch.cuda.is_available() else 'CPU'}\n",
        "\n",
        "ЦЕЛЬ:\n",
        "Получить максимальную метрику на русскоязычных бенчмарках\n",
        "GQA-ru и MMBench-ru\n",
        "\n",
        "{'='*60}\n",
        "ИСПОЛЬЗОВАННЫЕ ДАТАСЕТЫ:\n",
        "{'='*60}\n",
        "1. LLaVA-Instruct-ru (обучение):\n",
        "   • Примеров: {len(datasets.get('instruct', []))}\n",
        "   • Тип: Инструктивные диалоги с изображениями\n",
        "\n",
        "2. GQA-ru (тестирование):\n",
        "   • Примеров: {len(datasets.get('gqa', []))}\n",
        "   • Тип: Вопросы по изображениям\n",
        "\n",
        "3. MMBench-ru (тестирование):\n",
        "   • Примеров: {len(datasets.get('mmbench', []))}\n",
        "   • Тип: Мультимодальные вопросы\n",
        "\n",
        "{'='*60}\n",
        "РЕЗУЛЬТАТЫ ОЦЕНКИ:\n",
        "{'='*60}\"\"\"\n",
        "\n",
        "    for benchmark, metrics in results.items():\n",
        "        report += f\"\"\"\n",
        "{benchmark}:\n",
        "  • Точность: {metrics['accuracy']:.2f}%\n",
        "  • Правильных ответов: {metrics['correct']}/{metrics['total_samples']}\n",
        "  • Статус: {'✅ Хорошо' if metrics['accuracy'] > 60 else '⚠️ Нужно улучшение' if metrics['accuracy'] > 30 else '❌ Плохо'}\"\"\"\n",
        "\n",
        "    report += f\"\"\"\n",
        "\n",
        "{'='*60}\n",
        "РЕКОМЕНДАЦИИ ПО УЛУЧШЕНИЮ:\n",
        "{'='*60}\n",
        "{suggestions}\n",
        "\n",
        "{'='*60}\n",
        "СЛЕДУЮЩИЕ ШАГИ:\n",
        "{'='*60}\n",
        "1. Увеличить размер тренировочных данных\n",
        "2. Использовать полную версию датасетов\n",
        "3. Применить продвинутые техники аугментации\n",
        "4. Настроить гиперпараметры обучения\n",
        "5. Протестировать на реальных изображениях\n",
        "\n",
        "{'='*60}\n",
        "АРХИТЕКТУРА РЕШЕНИЯ:\n",
        "{'='*60}\n",
        "• Базовая модель: LLaVA-Saiga-8B\n",
        "• Метод адаптации: LoRA (Low-Rank Adaptation)\n",
        "• Точность: float16\n",
        "• Размер ранга LoRA: 16\n",
        "• Обучение: Инструктивное fine-tuning\n",
        "\"\"\"\n",
        "\n",
        "    # Сохраняем отчет\n",
        "    with open(\"project_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(report)\n",
        "\n",
        "    print(report)\n",
        "    return report\n",
        "\n",
        "# 8. ОСНОВНОЙ СЦЕНАРИЙ ВЫПОЛНЕНИЯ\n",
        "def main(training_mode: bool = False, max_samples: int = 20):\n",
        "    \"\"\"Полный пайплайн проекта\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"ЗАПУСК ПРОЕКТА: Обучение русскоязычной VLM модели\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Проверяем доступность GPU\n",
        "    print(f\"\\nПРОВЕРКА ОБОРУДОВАНИЯ:\")\n",
        "    print(f\"  GPU доступен: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"  Память GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    else:\n",
        "        print(\"  ⚠️  Внимание: GPU не найден, обучение может быть медленным\")\n",
        "\n",
        "    # Шаг 1: Загрузка данных\n",
        "    datasets = load_all_datasets()\n",
        "\n",
        "    # Шаг 2: Инициализация процессора\n",
        "    processor = LLaVADataProcessor()\n",
        "\n",
        "    # Шаг 3: Обучение или загрузка модели\n",
        "    if training_mode:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"РЕЖИМ: ОБУЧЕНИЕ НОВОЙ МОДЕЛИ\")\n",
        "        print(\"=\"*60)\n",
        "        model, processor = train_vlm_model(datasets, processor, epochs=1)\n",
        "        if model is None:\n",
        "            print(\"Не удалось обучить модель, используем предобученную\")\n",
        "            model = setup_model_with_lora(use_lora=False)\n",
        "    else:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"РЕЖИМ: ТЕСТИРОВАНИЕ ПРЕДОБУЧЕННОЙ МОДЕЛИ\")\n",
        "        print(\"=\"*60)\n",
        "        model = setup_model_with_lora(use_lora=False)\n",
        "\n",
        "    # Шаг 4: Оценка на бенчмарках\n",
        "    results = evaluate_on_benchmarks(model, processor, datasets, max_samples)\n",
        "\n",
        "    # Шаг 5: Анализ и оптимизация\n",
        "    optimizer = BenchmarkOptimizer()\n",
        "    error_examples = optimizer.analyze_errors(model, processor, datasets, num_examples=5)\n",
        "    suggestions = optimizer.get_improvement_suggestions(results, error_examples)\n",
        "\n",
        "    # Шаг 6: Создание отчета\n",
        "    report = create_report(results, datasets, suggestions)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ПРОЕКТ УСПЕШНО ЗАВЕРШЕН!\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Созданные файлы:\")\n",
        "    print(\"  1. benchmark_results.json - результаты оценки\")\n",
        "    print(\"  2. error_analysis.json - анализ ошибок\")\n",
        "    print(\"  3. project_report.txt - полный отчет\")\n",
        "    if training_mode:\n",
        "        print(\"  4. llava-ru-finetuned-final/ - дообученная модель\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return model, processor, results\n",
        "\n",
        "# 9. ЗАПУСК ПРОЕКТА\n",
        "if __name__ == \"__main__\":\n",
        "    # Параметры запуска\n",
        "    TRAINING_MODE = False  # Поставьте True для обучения модели\n",
        "    MAX_SAMPLES = 20      # Количество примеров для оценки\n",
        "\n",
        "    try:\n",
        "        model, processor, results = main(\n",
        "            training_mode=TRAINING_MODE,\n",
        "            max_samples=MAX_SAMPLES\n",
        "        )\n",
        "\n",
        "        # Дополнительная информация\n",
        "        print(\"\\nДОПОЛНИТЕЛЬНАЯ ИНФОРМАЦИЯ:\")\n",
        "        print(f\"  Размер модели: {sum(p.numel() for p in model.parameters()):,} параметров\")\n",
        "        print(f\"  Точность лучшего бенчмарка: {max(r['accuracy'] for r in results.values()):.2f}%\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ ОШИБКА ВЫПОЛНЕНИЯ: {e}\")\n",
        "        print(\"\\nСОВЕТЫ ПО УСТРАНЕНИЮ:\")\n",
        "        print(\"1. Проверьте подключение к интернету\")\n",
        "        print(\"2. Убедитесь, что есть достаточно памяти GPU (минимум 8GB)\")\n",
        "        print(\"3. Уменьшите MAX_SAMPLES если памяти мало\")\n",
        "        print(\"4. Используйте Google Colab с GPU T4 или лучше\")\n",
        "\n",
        "        # Минимальный отчет при ошибке\n",
        "        try:\n",
        "            with open(\"error_log.txt\", \"w\") as f:\n",
        "                import traceback\n",
        "                f.write(traceback.format_exc())\n",
        "            print(\"Лог ошибки сохранен в error_log.txt\")\n",
        "        except:\n",
        "            pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1a4a86af34e646e4a3e69444b446981d",
            "261ae4c77fcf496aaa64ff2ec7bfefe0",
            "ce28e9ce01c8446999c9e094daa9878e",
            "799ed81b7def4b7d80a3d5e763dcbfac",
            "afc656c70e06442d9341d73e63f26aaa",
            "680992ab40544f7cb30708188d185055",
            "136aab397abd44749e8c1b941c33721b",
            "5e475ce081dc47f7b04f3e7aecb29008",
            "e79043955717472c9f9688ec9d0ed10c",
            "0b6e5208bd4346778497a5b1e290f17b",
            "4452c300bdbb4642b4fd9656befef648",
            "192f6203fc574dc29925efba5e052ac6",
            "7112cea4745b46a4adfb74f43664b131",
            "e3e02585ca294112b662818a355dab7a",
            "f0f1f25e65404df1a2115ff118e2a031",
            "21955a49cc574711bde14f5969e72d9a",
            "225fe8becac94f54972e5db1e9ed318e",
            "32842a287f6845c99f0a12d5e414c7e0",
            "c7a09385a4a6488084c0ff00dd4953b7",
            "74a856c2bba74ff7bf8014a502f4d921",
            "c6a6c65b319f4632b253534e7124942f",
            "9a4bf78538344c768fea92558e2ee31f",
            "b9b428efbb424ab7a182930eca73b7dc",
            "7a93ac12333f4417ac4681119fed374d",
            "1c5c213913394993bad5284b44f4871d",
            "2a7b4ff9371f4d999601b6df4e2cd4f3",
            "48a7c6dddd474529bd2f596de274534b",
            "12b8a71f4a474a538e5e45970ac14b6f",
            "a2eaa3b35994454aa3e29ad8612eb7e3",
            "efb89b03d8b94d7497739cbe4b2207ab",
            "b75a60a724814ba09788f3996d13396c",
            "6fac246c8f3944ebb96288901ba5385c",
            "da5d86be2abd4704a19373777cdc064d",
            "da3f4ff3e51d4419b462b19423e45ce4",
            "9740b90436c2414eb626566540f0a51f",
            "78cb92180ebd42c188d39a554b45886d",
            "f88e712da66945a8aa8bbaa9a474821a",
            "726df2129d2f4f90905b4e0dc7227dbd",
            "51e3c7de0a3b443baa7b1f04b263ed5f",
            "2fa4bb28f74f4892bd05c7e7cca88883",
            "8e828cea849e49c9adb29eb8aa5c2ca6",
            "84170f17cc5543fa89855af354ab5d7a",
            "43e1ed709a9f46c2ba2b698109bbcac1",
            "c5406c348e5c48899305cb16bfd3b621",
            "3c7a497ab26b4f9fa0e0d943e41d70c6",
            "bb968a461c474c51bca9e60ccf07cb5a",
            "71a6befdcb8d4e01b29bb939aef8e911",
            "55d3d222ba7944dd8d65fa0b909cf27b",
            "defe162abdab4f0db28ec8e462f249ed",
            "8d8b8b33c5c64c04aa0568dc023d97b2",
            "49c0149ee9614b73a0d1d55ca8c10f32",
            "02a30486269b4cba8cd0944050b40290",
            "5490cd8c06ec483ea9fabfe49fa2c192",
            "8dc583d1ed8d4da382e933bb0fb9fedd",
            "a2cab13218634bba9212b5ac64bfea19",
            "dbce16ca9f344465b237cf641b707327",
            "86eb53679c2140de940c6582d53cdeb0",
            "75442306c7654a5e93cd6da7fe5b70e5",
            "e0dc939be1594f4e827c629b549871f5",
            "462e966709344bdcb1e0d7df4addf84a",
            "73b0eda15e494b959f395e0cbfbf1878",
            "cfbf0e2f829a4d0cbe6e06b9ce567cf6",
            "663231bfd1084b5bb651a6dd985bdde8",
            "e45d999869d444d0b13153a782234a8d",
            "4e0f30f57b7f476d8e92079a9dd32e02",
            "84d1c464a2e34737ab02ff514a095bcb"
          ]
        },
        "id": "hs9FSd-jv0NH",
        "outputId": "36d5e148-5333-426e-942c-2ad2b7643565"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ИНИЦИАЛИЗАЦИЯ ПРОЕКТА VLM НА РУССКОМ ЯЗЫКЕ\n",
            "============================================================\n",
            "============================================================\n",
            "ЗАПУСК ПРОЕКТА: Обучение русскоязычной VLM модели\n",
            "============================================================\n",
            "\n",
            "ПРОВЕРКА ОБОРУДОВАНИЯ:\n",
            "  GPU доступен: True\n",
            "  GPU: Tesla T4\n",
            "  Память GPU: 15.83 GB\n",
            "\n",
            "1. ЗАГРУЗКА ДАТАСЕТОВ...\n",
            "Загрузка LLaVA-Instruct-ru...\n",
            "  Загружено: 109905 примеров\n",
            "\n",
            "Загрузка GQA-ru...\n",
            "  Загружено: 12216 примеров\n",
            "\n",
            "Загрузка MMBench-ru...\n",
            "  Загружено: 3910 примеров\n",
            "  Ограничено до: 1000 примеров для демо\n",
            "\n",
            "2. ИНИЦИАЛИЗАЦИЯ ПРОЦЕССОРА (deepvk/llava-saiga-8b)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Процессор успешно загружен\n",
            "\n",
            "============================================================\n",
            "РЕЖИМ: ТЕСТИРОВАНИЕ ПРЕДОБУЧЕННОЙ МОДЕЛИ\n",
            "============================================================\n",
            "\n",
            "3. ЗАГРУЗКА МОДЕЛИ deepvk/llava-saiga-8b...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a4a86af34e646e4a3e69444b446981d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "192f6203fc574dc29925efba5e052ac6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9b428efbb424ab7a182930eca73b7dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da3f4ff3e51d4419b462b19423e45ce4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c7a497ab26b4f9fa0e0d943e41d70c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbce16ca9f344465b237cf641b707327"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Модель загружена в 4-битном режиме\n",
            "  Модель успешно загружена на устройство: cuda:0\n",
            "\n",
            "5. ОЦЕНКА НА БЕНЧМАРКАХ (20 примеров каждый)...\n",
            "\n",
            "  ОЦЕНКА НА GQA-ru...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GQA-ru:   0%|          | 0/20 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "GQA-ru:  10%|█         | 2/20 [00:04<00:39,  2.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Пример 2:\n",
            "      Вопрос: Кто носит платье?...\n",
            "      Ожидалось: женщины\n",
            "      Получено: На картинке изображена женщина, которая носит платье.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GQA-ru:  25%|██▌       | 5/20 [00:14<00:49,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Пример 5:\n",
            "      Вопрос: Какой высоты стул внизу фотографии?...\n",
            "      Ожидалось: низкий\n",
            "      Получено: На фотографии стул не виден, поэтому нельзя точно определить его высоту. Однако, учитывая, что стул находится на верхушке лестницы, можно предположить, что он может быть средней высоты, чтобы обеспечить удобство при использовании лестницы.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GQA-ru: 100%|██████████| 20/20 [00:41<00:00,  2.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  ОЦЕНКА НА MMBench-ru...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MMBench-ru:   5%|▌         | 1/20 [00:09<02:54,  9.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Пример 1:\n",
            "      Вопрос: Какая часть яблони может вырасти в новое дерево?...\n",
            "      Ожидалось: A\n",
            "      Получено: В зависимости от условий и способов размножения, часть яблони может вырасти в новое дерево. Например, если на дереве есть плоды, которые упали и остались на земле, они могут послужить источником для нового дерева. Также, если на дереве есть ветки, которые могут быть отделены и посажены в землю, это также способ для создания нового дерева. Важно учитывать, что для успешного размнож\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMMBench-ru:  10%|█         | 2/20 [00:10<01:21,  4.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Пример 2:\n",
            "      Вопрос: Из какого материала сделана эта лопатка?...\n",
            "      Ожидалось: A\n",
            "      Получено: Лопатка сделана из дерева.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMMBench-ru:  15%|█▌        | 3/20 [00:12<00:57,  3.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Пример 3:\n",
            "      Вопрос: Это место многолюдное?...\n",
            "      Ожидалось: A\n",
            "      Получено: На изображении не видно множества людей, скорее всего это тихое место.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMMBench-ru:  20%|██        | 4/20 [00:20<01:24,  5.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Пример 4:\n",
            "      Вопрос: Какое изображение более красочное?...\n",
            "      Ожидалось: A\n",
            "      Получено: На изображении, где изображены разноцветные фрукты, цветы и трава, более красочное. Это создает яркое и живописное визуальное впечатление. В то время как на другом изображении, где изображены разные виды фруктов, цветов и травы, цветовая гамма может быть более нейтральной и менее яркой.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMMBench-ru:  25%|██▌       | 5/20 [00:24<01:11,  4.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Пример 5:\n",
            "      Вопрос: Какая картинка ярче?...\n",
            "      Ожидалось: A\n",
            "      Получено: На изображении ярче изображение с цветами, так как цветы обычно имеют яркие и насыщенные цвета, в то время как фон может быть более нейтральным.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MMBench-ru: 100%|██████████| 20/20 [01:45<00:00,  5.29s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "РЕЗУЛЬТАТЫ ОЦЕНКИ:\n",
            "  GQA-ru: 5/20 (25.00%)\n",
            "  MMBench-ru: 0/20 (0.00%)\n",
            "==================================================\n",
            "\n",
            "6. АНАЛИЗ ОШИБОК МОДЕЛИ...\n",
            "\n",
            "  Анализ GQA...\n",
            "    Ошибка #1:\n",
            "      Вопрос: Кто носит платье?...\n",
            "      Ожидалось: женщины\n",
            "      Получено: На картинке изображена женщина, которая носит платье....\n",
            "    Ошибка #2:\n",
            "      Вопрос: Какой высоты стул внизу фотографии?...\n",
            "      Ожидалось: низкий\n",
            "      Получено: На фотографии стул не виден, поэтому нельзя точно определить...\n",
            "\n",
            "  Анализ MMBENCH...\n",
            "    Ошибка #3:\n",
            "      Вопрос: Какая часть яблони может вырасти в новое дерево?...\n",
            "      Ожидалось: A\n",
            "      Получено: В зависимости от условий и способов размножения, часть яблон...\n",
            "    Ошибка #4:\n",
            "      Вопрос: Из какого материала сделана эта лопатка?...\n",
            "      Ожидалось: A\n",
            "      Получено: Лопатка сделана из дерева....\n",
            "    Ошибка #5:\n",
            "      Вопрос: Это место многолюдное?...\n",
            "      Ожидалось: A\n",
            "      Получено: На изображении не видно множества людей, скорее всего это ти...\n",
            "    Ошибка #6:\n",
            "      Вопрос: Какое изображение более красочное?...\n",
            "      Ожидалось: A\n",
            "      Получено: На изображении, где изображены разноцветные фрукты, цветы и ...\n",
            "    Ошибка #7:\n",
            "      Вопрос: Какая картинка ярче?...\n",
            "      Ожидалось: A\n",
            "      Получено: На изображении ярче изображение с цветами, так как цветы обы...\n",
            "\n",
            "  Сохранено 7 примеров ошибок в error_analysis.json\n",
            "\n",
            "============================================================\n",
            "ФИНАЛЬНЫЙ ОТЧЕТ ПРОЕКТА\n",
            "============================================================\n",
            "Дата: 2026-01-11 16:30:50\n",
            "Устройство: GPU\n",
            "\n",
            "ЦЕЛЬ: \n",
            "Получить максимальную метрику на русскоязычных бенчмарках \n",
            "GQA-ru и MMBench-ru\n",
            "\n",
            "============================================================\n",
            "ИСПОЛЬЗОВАННЫЕ ДАТАСЕТЫ:\n",
            "============================================================\n",
            "1. LLaVA-Instruct-ru (обучение): \n",
            "   • Примеров: 1000\n",
            "   • Тип: Инструктивные диалоги с изображениями\n",
            "\n",
            "2. GQA-ru (тестирование):\n",
            "   • Примеров: 12216\n",
            "   • Тип: Вопросы по изображениям\n",
            "\n",
            "3. MMBench-ru (тестирование):\n",
            "   • Примеров: 3910\n",
            "   • Тип: Мультимодальные вопросы\n",
            "\n",
            "============================================================\n",
            "РЕЗУЛЬТАТЫ ОЦЕНКИ:\n",
            "============================================================\n",
            "GQA-ru:\n",
            "  • Точность: 25.00%\n",
            "  • Правильных ответов: 5/20\n",
            "  • Статус: ❌ Плохо\n",
            "MMBench-ru:\n",
            "  • Точность: 0.00%\n",
            "  • Правильных ответов: 0/20\n",
            "  • Статус: ❌ Плохо\n",
            "\n",
            "============================================================\n",
            "РЕКОМЕНДАЦИИ ПО УЛУЧШЕНИЮ:\n",
            "============================================================\n",
            "1. Увеличить время обучения на LLaVA-Instruct-ru до 3-5 эпох\n",
            "2. Добавить data augmentation для текстовых вопросов\n",
            "3. Дообучить на разнообразных вопросах MMBench-ru\n",
            "5. Использовать ансамблирование нескольких моделей\n",
            "6. Настроить температуру генерации (0.05 для точности, 0.7 для креативности)\n",
            "7. Увеличить размер ранга LoRA (r=32 или r=64)\n",
            "\n",
            "============================================================\n",
            "СЛЕДУЮЩИЕ ШАГИ:\n",
            "============================================================\n",
            "1. Увеличить размер тренировочных данных\n",
            "2. Использовать полную версию датасетов\n",
            "3. Применить продвинутые техники аугментации\n",
            "4. Настроить гиперпараметры обучения\n",
            "5. Протестировать на реальных изображениях\n",
            "\n",
            "============================================================\n",
            "АРХИТЕКТУРА РЕШЕНИЯ:\n",
            "============================================================\n",
            "• Базовая модель: LLaVA-Saiga-8B\n",
            "• Метод адаптации: LoRA (Low-Rank Adaptation)\n",
            "• Точность: float16\n",
            "• Размер ранга LoRA: 16\n",
            "• Обучение: Инструктивное fine-tuning\n",
            "\n",
            "\n",
            "============================================================\n",
            "ПРОЕКТ УСПЕШНО ЗАВЕРШЕН!\n",
            "============================================================\n",
            "Созданные файлы:\n",
            "  1. benchmark_results.json - результаты оценки\n",
            "  2. error_analysis.json - анализ ошибок\n",
            "  3. project_report.txt - полный отчет\n",
            "============================================================\n",
            "\n",
            "ДОПОЛНИТЕЛЬНАЯ ИНФОРМАЦИЯ:\n",
            "  Размер модели: 4,704,131,072 параметров\n",
            "  Точность лучшего бенчмарка: 25.00%\n"
          ]
        }
      ]
    }
  ]
}